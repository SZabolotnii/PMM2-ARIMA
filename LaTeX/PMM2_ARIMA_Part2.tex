% ============================================
% PART 2: METHODOLOGY AND MONTE CARLO DESIGN
% ============================================
% This file contains Section 2 (Methodology)
% To be compiled with Part 1 (PMM2_ARIMA_Part1.tex)
% ============================================

\section{Методологія}
\label{sec:methodology}

\subsection{Специфікація моделі ARIMA}
\label{subsec:arima_specification}

Розглянемо загальну модель ARIMA(p,d,q) для стаціонарного часового ряду $\{y_t\}$, $t=1,\ldots,T$:

\begin{equation}
\label{eq:arima_general}
\Phi(B)(1-B)^d y_t = \Theta(B) \varepsilon_t
\end{equation}

\noindent де:
\begin{itemize}
    \item $B$ --- оператор зсуву назад: $By_t = y_{t-1}$
    \item $(1-B)^d$ --- оператор диференціювання порядку $d$
    \item $\Phi(B) = 1 - \phi_1 B - \phi_2 B^2 - \ldots - \phi_p B^p$ --- авторегресійний поліном порядку $p$
    \item $\Theta(B) = 1 + \theta_1 B + \theta_2 B^2 + \ldots + \theta_q B^q$ --- поліном ковзного середнього порядку $q$
    \item $\{\varepsilon_t\}$ --- послідовність інновацій (білий шум)
\end{itemize}

\paragraph{Стандартні припущення.}

\begin{enumerate}
    \item \textbf{Стаціонарність:} Корені рівняння $\Phi(z)=0$ знаходяться поза одиничним колом.
    \item \textbf{Інвертованість:} Корені рівняння $\Theta(z)=0$ знаходяться поза одиничним колом.
    \item \textbf{Незалежність інновацій:} $\varepsilon_t$ --- незалежні однаково розподілені випадкові величини.
    \item \textbf{Центрованість:} $E[\varepsilon_t] = 0$.
    \item \textbf{Скінченна дисперсія:} $\text{Var}[\varepsilon_t] = \sigma^2 < \infty$.
\end{enumerate}

\paragraph{Ключове розширення.}

На відміну від класичного підходу, ми \textbf{не припускаємо гаусовість} інновацій. Натомість, ми дозволяємо:

\begin{equation}
\label{eq:non_gaussian}
\varepsilon_t \sim F(\cdot), \quad \text{де} \quad E[\varepsilon_t] = 0, \quad \text{Var}[\varepsilon_t] = \sigma^2
\end{equation}

\noindent і розподіл $F$ може мати:
\begin{itemize}
    \item \textbf{Ненульовий коефіцієнт асиметрії:} $\gamma_3 = \kappa_3/\sigma^3 \neq 0$
    \item \textbf{Ненульовий коефіцієнт ексцесу:} $\gamma_4 = \kappa_4/\sigma^4 \neq 0$
\end{itemize}

\noindent де $\kappa_3$ та $\kappa_4$ --- третій та четвертий кумулянти відповідно.

\subsection{Класичні методи оцінювання}
\label{subsec:classical_methods}

\subsubsection{Метод умовної суми квадратів (CSS)}
\label{subsubsec:css}

CSS мінімізує суму квадратів умовних залишків:

\begin{equation}
\label{eq:css_objective}
\min_{\phi,\theta} S(\phi,\theta) = \sum_{t=p+1}^{T} \hat{\varepsilon}_t^2(\phi,\theta)
\end{equation}

\noindent де залишки обчислюються рекурсивно:

\begin{equation}
\label{eq:css_residuals}
\hat{\varepsilon}_t = y_t - \phi_1 y_{t-1} - \ldots - \phi_p y_{t-p} - \theta_1 \hat{\varepsilon}_{t-1} - \ldots - \theta_q \hat{\varepsilon}_{t-q}
\end{equation}

\paragraph{Властивості CSS під негаусовістю:}
\begin{itemize}
    \item[✓] Спроможність (під умовами регулярності)
    \item[✓] Асимптотична нормальність
    \item[✗] \textbf{Втрата ефективності} при $\gamma_3 \neq 0$
\end{itemize}

\subsubsection{Метод максимальної правдоподібності (MLE)}
\label{subsubsec:mle}

Під припущенням гаусовості, $\varepsilon_t \sim \mathcal{N}(0,\sigma^2)$, логарифм функції правдоподібності має вигляд:

\begin{equation}
\label{eq:mle_objective}
\log L(\phi,\theta,\sigma^2) = -\frac{T}{2}\log(2\pi\sigma^2) - \frac{1}{2\sigma^2}\sum_{t=1}^{T} \varepsilon_t^2
\end{equation}

MLE максимізує~\eqref{eq:mle_objective} відносно параметрів $(\phi,\theta,\sigma^2)$.

\paragraph{Властивості MLE під негаусовістю:}
\begin{itemize}
    \item[✓] Квазі-максимальна правдоподібність залишається спроможною
    \item[✗] \textbf{Більше не асимптотично ефективна} коли $F \neq \mathcal{N}(0,\sigma^2)$
    \item[✗] Стандартні похибки вимагають корекції (sandwich estimator)
\end{itemize}

\subsection{Метод максимізації поліномів другого порядку (PMM2)}
\label{subsec:pmm2_method}

\subsubsection{Концептуальна основа}
\label{subsubsec:pmm2_conceptual}

PMM2 базується на ідеї максимізації дисперсії стохастичного полінома другого порядку. Для задачі оцінювання параметрів це еквівалентно розв'язанню системи нелінійних рівнянь, що використовують інформацію з моментів вищих порядків залишків.

\paragraph{Ключова інтуїція.} Коли інновації асиметричні ($\gamma_3 \neq 0$), третій та четвертий моменти залишків містять додаткову інформацію про параметри моделі, яку OLS/CSS не використовує. PMM2 ефективно інкорпорує цю інформацію через відповідне зважування залишків.

\subsubsection{PMM2 для AR(p) моделі}
\label{subsubsec:pmm2_ar}

Розглянемо спочатку простіший випадок чистої AR(p) моделі:

\begin{equation}
\label{eq:ar_model}
x_t = \phi_1 x_{t-1} + \phi_2 x_{t-2} + \ldots + \phi_p x_{t-p} + \varepsilon_t
\end{equation}

\noindent де $x_t$ --- стаціонарний ряд (отриманий після диференціювання).

\paragraph{Система PMM2 рівнянь.}

Для кожного параметра $\phi_j$, $j=1,\ldots,p$, PMM2 система має вигляд:

\begin{equation}
\label{eq:pmm2_system}
Z_j(\phi) = \sum_{t=p+1}^{T} x_{t-j} \left[ A \hat{\varepsilon}_t^2 + B \hat{\varepsilon}_t + C \right] = 0
\end{equation}

\noindent де коефіцієнти $A$, $B$, $C$ визначаються через центральні моменти залишків CSS/OLS оцінки:

\begin{align}
A &= m_3 \label{eq:coeff_A}\\
B &= m_4 - m_2^2 - 2m_3 y \label{eq:coeff_B}\\
C &= m_3 y^2 - (m_4 - m_2^2)y - m_2 m_3 \label{eq:coeff_C}
\end{align}

\noindent де:
\begin{itemize}
    \item $m_2 = \frac{1}{n}\sum_{t=1}^{n} \hat{\varepsilon}_t^2$ --- другий центральний момент
    \item $m_3 = \frac{1}{n}\sum_{t=1}^{n} \hat{\varepsilon}_t^3$ --- третій центральний момент
    \item $m_4 = \frac{1}{n}\sum_{t=1}^{n} \hat{\varepsilon}_t^4$ --- четвертий центральний момент
    \item $\hat{\varepsilon}_t$ --- залишки з початкової CSS/OLS оцінки
    \item $y = \frac{1}{n}\sum_{t=1}^{n} \hat{\varepsilon}_t$ (зазвичай $\approx 0$)
\end{itemize}

\paragraph{Якобіан системи PMM2.}

Для розв'язання нелінійної системи~\eqref{eq:pmm2_system} застосовується ітеративна процедура Ньютона-Рафсона. Якобіан системи має вигляд:

\begin{equation}
\label{eq:jacobian}
J_{ij}(\phi) = \frac{\partial Z_i}{\partial \phi_j} = \sum_{t=p+1}^{T} (2A\hat{\varepsilon}_t + B) x_{t-i} x_{t-j}
\end{equation}

\noindent де $\hat{\varepsilon}_t = x_t - \sum_{k=1}^{p} \phi_k x_{t-k}$.

\paragraph{Ітеративна процедура.}

Починаючи з початкового наближення $\phi^{(0)}$ (наприклад, CSS оцінки), параметри оновлюються за формулою:

\begin{equation}
\label{eq:newton_raphson}
\phi^{(r+1)} = \phi^{(r)} - J^{-1}(\phi^{(r)}) Z(\phi^{(r)})
\end{equation}

\noindent де $r$ --- номер ітерації. Процес продовжується до досягнення збіжності:

\begin{equation}
\label{eq:convergence_criteria}
\|Z(\phi^{(r)})\| < \varepsilon \quad \text{або} \quad \|\phi^{(r+1)} - \phi^{(r)}\| < \varepsilon
\end{equation}

\noindent де $\varepsilon$ --- заданий поріг точності (зазвичай $10^{-6}$).

\paragraph{Числова стабільність.}

Для забезпечення числової стабільності, особливо при малих вибірках, до діагоналі Якобіану додається регуляризаційний член:

\begin{equation}
\label{eq:regularization}
J_{\text{reg}} = J + \lambda I
\end{equation}

\noindent де $\lambda$ --- малий регуляризаційний параметр (зазвичай $10^{-8}$), $I$ --- одинична матриця.

\subsubsection{Асимптотичні властивості PMM2 оцінок}
\label{subsubsec:pmm2_asymptotic}

За умов регулярності, PMM2 оцінки для AR(p) моделі мають наступні асимптотичні властивості:

\begin{theorem}[Спроможність]
\label{thm:consistency}
Якщо істинні параметри належать внутрішності параметричного простору, інновації мають скінченні моменти до четвертого порядку включно, і модель коректно специфікована, то:
\begin{equation}
\hat{\phi}_{\text{PMM2}} \xrightarrow{p} \phi_0 \quad \text{при} \quad T \to \infty
\end{equation}
де $\phi_0$ --- вектор істинних параметрів, $\xrightarrow{p}$ означає збіжність за ймовірністю.
\end{theorem}

\begin{theorem}[Асимптотична нормальність]
\label{thm:asymptotic_normality}
За тих самих умов:
\begin{equation}
\label{eq:asymptotic_distribution}
\sqrt{T}(\hat{\phi}_{\text{PMM2}} - \phi_0) \xrightarrow{d} \mathcal{N}(0, V_{\text{PMM2}})
\end{equation}
де $\xrightarrow{d}$ означає збіжність за розподілом, і асимптотична коваріаційна матриця:
\begin{equation}
\label{eq:asymptotic_variance}
V_{\text{PMM2}} = \left(1 - \frac{\gamma_3^2}{2 + \gamma_4}\right) V_{\text{OLS}}
\end{equation}
де $V_{\text{OLS}}$ --- асимптотична коваріаційна матриця OLS оцінок.
\end{theorem}

\begin{corollary}[Відносна ефективність]
\label{cor:relative_efficiency}
Відносна ефективність PMM2 щодо OLS визначається як:
\begin{equation}
\label{eq:relative_efficiency_detailed}
RE = \frac{\text{Var}(\hat{\theta}_{\text{OLS}})}{\text{Var}(\hat{\theta}_{\text{PMM2}})} = \frac{1}{1 - \frac{\gamma_3^2}{2+\gamma_4}} = \frac{2 + \gamma_4}{2 + \gamma_4 - \gamma_3^2}
\end{equation}
\end{corollary}

\noindent \textbf{Висновок:} PMM2 оцінки є асимптотично нормальними з \textbf{меншою дисперсією} порівняно з OLS коли $\gamma_3 \neq 0$.

\subsection{PMM2 для ARIMA(p,d,q) моделей}
\label{subsec:pmm2_arima}

\subsubsection{Загальна стратегія}
\label{subsubsec:arima_strategy}

Для повної ARIMA(p,d,q) моделі застосовується наступний алгоритм:

\paragraph{Крок 1: Диференціювання.} Застосувати оператор диференціювання $d$ разів до вихідного ряду:
\begin{equation}
\label{eq:differencing}
x_t = (1-B)^d y_t
\end{equation}

\paragraph{Крок 2: Тест на стаціонарність.} Перевірити стаціонарність продиференційованого ряду $\{x_t\}$ за допомогою розширеного тесту Дікі-Фуллера (ADF):
\begin{equation}
\label{eq:adf_test}
\Delta x_t = \alpha + \beta t + \gamma x_{t-1} + \sum_{j=1}^{k} \delta_j \Delta x_{t-j} + \varepsilon_t
\end{equation}

Нульова гіпотеза $H_0: \gamma = 0$ (наявність одиничного кореня) відхиляється якщо p-value $< 0.05$.

\paragraph{Крок 3: Застосування PMM2 до стаціонарного ряду.} Залежно від структури моделі:

\begin{itemize}
    \item \textbf{Випадок A: AR(p) модель ($q=0$):} Застосувати PMM2 алгоритм з розділу~\ref{subsubsec:pmm2_ar} безпосередньо до стаціонарного ряду $\{x_t\}$.

    \item \textbf{Випадок B: MA(q) модель ($p=0$):} MA модель має вигляд:
    \begin{equation}
    \label{eq:ma_model}
    x_t = \varepsilon_t + \theta_1 \varepsilon_{t-1} + \ldots + \theta_q \varepsilon_{t-q}
    \end{equation}
    Для MA моделі застосовується двоетапна процедура:
    \begin{enumerate}
        \item Початкова CSS оцінка для отримання $\theta^{(0)}$
        \item Обчислення моментів залишків
        \item Формування PMM2 системи (аналогічно AR випадку)
        \item Ітеративне розв'язання
    \end{enumerate}

    \item \textbf{Випадок C: Повна ARMA(p,q) модель:} Використовується комбінований підхід:
    \begin{enumerate}
        \item Початкова CSS оцінка для отримання $(\phi^{(0)}, \theta^{(0)})$
        \item Фіксування MA параметрів, застосування PMM2 до AR частини
        \item Оновлення залишків
        \item Альтернативна оптимізація (якщо необхідно)
    \end{enumerate}
\end{itemize}

\subsubsection{Практична імплементація}
\label{subsubsec:practical_implementation}

Алгоритм~\ref{alg:pmm2_arima} представляє повну процедуру PMM2 для ARIMA(p,d,q) моделей.

\begin{algorithm}[H]
\caption{PMM2 для ARIMA(p,d,q) моделей}
\label{alg:pmm2_arima}
\begin{algorithmic}[1]
\REQUIRE Часовий ряд $y = \{y_1, \ldots, y_T\}$, параметри $(p,d,q)$
\ENSURE PMM2 оцінки параметрів $(\hat{\phi}, \hat{\theta})$

\STATE \textbf{Диференціювання:} $x \leftarrow (1-B)^d y$

\STATE \textbf{Перевірка стаціонарності:}
\IF{$\text{ADF\_test}(x).p\_value > 0.05$}
    \RETURN ``Ряд нестаціонарний, збільшіть $d$''
\ENDIF

\STATE \textbf{Ініціалізація (CSS):}
\STATE $(\phi^{(0)}, \theta^{(0)}) \leftarrow \text{CSS\_estimate}(x, p, q)$
\STATE $\hat{\varepsilon}^{(0)} \leftarrow$ обчислити залишки

\STATE \textbf{Обчислення моментів:}
\STATE $m_2 \leftarrow \text{mean}(\hat{\varepsilon}^2)$
\STATE $m_3 \leftarrow \text{mean}(\hat{\varepsilon}^3)$
\STATE $m_4 \leftarrow \text{mean}(\hat{\varepsilon}^4)$
\STATE $\gamma_3 \leftarrow m_3/m_2^{3/2}$
\STATE $\gamma_4 \leftarrow (m_4 - 3m_2^2)/m_2^2$

\STATE \textbf{Перевірка асиметрії:}
\IF{$|\gamma_3| < 0.1$}
    \STATE Повідомити ``Низька асиметрія, PMM2 може не дати переваги''
\ENDIF

\STATE \textbf{PMM2 ітерації:}
\STATE $\phi^{(r)} \leftarrow \phi^{(0)}$
\FOR{$r = 1$ \TO $\text{max\_iter}$}
    \STATE Обчислити коефіцієнти $A, B, C$ (формули~\ref{eq:coeff_A}--\ref{eq:coeff_C})
    \STATE Сформувати $Z(\phi^{(r)})$ (формула~\ref{eq:pmm2_system})
    \IF{$\|Z(\phi^{(r)})\| < \varepsilon$}
        \STATE \textbf{break} (збіжність досягнута)
    \ENDIF
    \STATE Обчислити Якобіан $J(\phi^{(r)})$ (формула~\ref{eq:jacobian})
    \STATE Додати регуляризацію: $J \leftarrow J + \lambda I$
    \STATE Розв'язати: $\delta = J^{-1}Z$
    \STATE Оновити: $\phi^{(r+1)} = \phi^{(r)} - \delta$
\ENDFOR

\STATE \textbf{Обчислення статистик:}
\STATE $\hat{\varepsilon}_{\text{PMM2}} \leftarrow$ обчислити фінальні залишки
\STATE $\text{Var}_{\text{CSS}} \leftarrow \text{var}(\hat{\varepsilon}^{(0)})$
\STATE $\text{Var}_{\text{PMM2}} \leftarrow \text{var}(\hat{\varepsilon}_{\text{PMM2}})$
\STATE $\text{Variance\_Reduction} \leftarrow (1 - \text{Var}_{\text{PMM2}}/\text{Var}_{\text{CSS}}) \times 100\%$

\RETURN $\{\hat{\phi}_{\text{PMM2}}, \hat{\theta}_{\text{PMM2}}, \text{статистики}, \text{моменти}\}$
\end{algorithmic}
\end{algorithm}

\subsection{Дизайн Monte Carlo експерименту}
\label{subsec:monte_carlo_design}

\subsubsection{Цілі симуляційного дослідження}
\label{subsubsec:simulation_goals}

Monte Carlo експерименти проводяться для:
\begin{enumerate}
    \item Валідації асимптотичних результатів на скінченних вибірках
    \item Порівняння PMM2 з класичними методами (CSS, OLS)
    \item Дослідження впливу розміру вибірки
    \item Аналізу ефективності для різних розподілів інновацій
\end{enumerate}

\subsubsection{Параметри симуляцій}
\label{subsubsec:simulation_parameters}

\paragraph{Моделі.} Основна увага приділяється простішому випадку для чистоти експерименту:
\begin{itemize}
    \item \textbf{ARIMA(1,1,0):} $\Delta y_t = \phi_1 \Delta y_{t-1} + \varepsilon_t$ з $\phi_1 = 0.5$
\end{itemize}

\paragraph{Розміри вибірки:}
$$N \in \{100, 200, 500, 1000\}$$

\paragraph{Розподіли інновацій:}

\begin{enumerate}
    \item \textbf{Gaussian (контроль):}
    $$\varepsilon_t \sim \mathcal{N}(0, 1)$$
    Параметри: $\gamma_3 = 0$, $\gamma_4 = 0$

    \item \textbf{Gamma(2,1) --- помірна асиметрія:}
    $$\varepsilon_t \sim \Gamma(2, 1) - 2$$
    Теоретичні параметри: $\gamma_3 \approx 1.41$, $\gamma_4 \approx 3.0$

    \item \textbf{Lognormal --- сильна асиметрія:}
    $$\varepsilon_t \sim \text{LogNormal}(0, 0.5) - \text{mean}$$
    Теоретичні параметри: $\gamma_3 \approx 2.0$, $\gamma_4 \approx 6.0$

    \item \textbf{Chi-squared(3) --- помірна асиметрія:}
    $$\varepsilon_t \sim \chi^2(3) - 3$$
    Теоретичні параметри: $\gamma_3 \approx 1.63$, $\gamma_4 \approx 4.0$
\end{enumerate}

Всі розподіли центруються (віднімається середнє) для забезпечення $E[\varepsilon_t] = 0$.

\paragraph{Кількість повторень:}
$$M = 2000 \text{ симуляцій на кожну конфігурацію}$$

Загальна кількість симуляцій: $4 \text{ розподіли} \times 4 \text{ розміри} \times 2000 = 32{,}000$.

\subsubsection{Процедура генерації даних}
\label{subsubsec:data_generation}

Для кожної симуляції $m = 1, \ldots, M$:

\begin{enumerate}
    \item \textbf{Генерація інновацій:} Згенерувати $\{\varepsilon_t^{(m)}\}_{t=1}^{N}$ з обраного розподілу та центрувати.

    \item \textbf{Генерація AR(1) процесу:} Використовуючи стаціонарну ініціалізацію:
    $$x_0^{(m)} \sim \mathcal{N}\left(0, \frac{\sigma^2}{1-\phi_1^2}\right)$$
    Рекурсивно:
    $$x_t^{(m)} = \phi_1 x_{t-1}^{(m)} + \varepsilon_t^{(m)}, \quad t = 1, \ldots, N$$

    \item \textbf{Інтегрування (для $d=1$):}
    $$y_t^{(m)} = \sum_{k=1}^{t} x_k^{(m)}$$
\end{enumerate}

\subsubsection{Оцінювання та метрики}
\label{subsubsec:estimation_metrics}

Для кожної симуляції:
\begin{enumerate}
    \item \textbf{CSS оцінка:} $\hat{\phi}_{\text{CSS}}^{(m)}$
    \item \textbf{PMM2 оцінка:} $\hat{\phi}_{\text{PMM2}}^{(m)}$
\end{enumerate}

\paragraph{Обчислювані статистики (по $M$ симуляціях):}

\begin{itemize}
    \item \textbf{Зміщення (Bias):}
    $$\text{Bias}(\hat{\phi}) = \frac{1}{M}\sum_{m=1}^{M} \hat{\phi}^{(m)} - \phi_0$$

    \item \textbf{Дисперсія:}
    $$\text{Var}(\hat{\phi}) = \frac{1}{M-1}\sum_{m=1}^{M} \left(\hat{\phi}^{(m)} - \overline{\hat{\phi}}\right)^2$$
    де $\overline{\hat{\phi}} = \frac{1}{M}\sum_{m=1}^{M} \hat{\phi}^{(m)}$.

    \item \textbf{Середньоквадратична похибка (MSE):}
    $$\text{MSE}(\hat{\phi}) = \text{Bias}^2(\hat{\phi}) + \text{Var}(\hat{\phi})$$

    \item \textbf{Відносна ефективність (емпірична):}
    $$\widehat{RE} = \frac{\text{Var}(\hat{\phi}_{\text{CSS}})}{\text{Var}(\hat{\phi}_{\text{PMM2}})}$$

    \item \textbf{Відсоток зменшення дисперсії:}
    $$\text{VarRed\%} = \left(1 - \frac{\text{Var}(\hat{\phi}_{\text{PMM2}})}{\text{Var}(\hat{\phi}_{\text{CSS}})}\right) \times 100\%$$
\end{itemize}

\subsubsection{Програмна імплементація}
\label{subsubsec:software_implementation}

Всі симуляції проведено в Python 3.9+ з використанням бібліотек:
\begin{itemize}
    \item \textbf{NumPy 1.24+} --- числові обчислення
    \item \textbf{SciPy 1.10+} --- оптимізація та статистичні розподіли
    \item \textbf{statsmodels 0.14+} --- ADF тест та порівняння з класичними методами
    \item \textbf{pandas 2.0+} --- управління даними
    \item \textbf{matplotlib/seaborn} --- візуалізація
\end{itemize}

Код доступний у відкритому доступі для забезпечення відтворюваності результатів\footnote{Репозиторій: \url{https://github.com/SZabolotnii/PMM2-ARIMA}}.

% End of Part 2
