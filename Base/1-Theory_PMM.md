### **Частина 1: Розділ 2.1 - Загальна постановка задачі**

Ця частина вводить основні поняття та позначення, що використовуються в усьому розділі. Вона визначає загальну регресійну модель як суму детермінованої та випадкової складових.

---

### **РОЗДІЛ 2. СИНТЕЗ МОДЕЛЕЙ ТА МЕТОДІВ (АЛГОРИТМІВ) ПОЛІНОМІАЛЬНОГО ОЦІНЮВАННЯ ПАРАМЕТРІВ РЕГРЕСІЇ**

#### **2.1 Загальна постановка задачі**

Нехай є регресійна модель спостережень у вигляді суми детермінованої і випадкової складової

$$
y_v = R_v(\theta, X) + \xi_v, \quad v = \overline{1, N},
\tag{2.1}
$$

що описує залежність значень цільової змінної $Y = \{y_1, y_2, ..., y_N\}$ від в загальному випадку множини $X$ незалежних змінних (P-факторного регресора), сукупність яких можна представити у вигляді матриці розмірності PxN виду

$$
X = \begin{pmatrix}
x_{1,1} & x_{1,2} & \cdots & x_{1,N} \\
x_{2,1} & x_{2,2} & \cdots & x_{2,N} \\
\vdots & \vdots & \ddots & \vdots \\
x_{P,1} & x_{P,2} & \cdots & x_{P,N}
\end{pmatrix}
$$

Детермінована складова $R_v(\theta, X)$ містить Q-мірний вектор інформативних параметрів $\theta = \{a_0, a_1, ..., a_{Q-1}\}$.

Відліки випадкової складової (помилки) регресійної моделі $\xi_v$ представляють собою послідовність незалежних і однаково-розподілених випадкових величин, що володіють властивістю центрованості ($E\{\xi\} = 0$) та гомоскедактичності $D\{\xi\} = const$. Імовірнісні властивості помилки регресійної моделі суттєво відрізняються від гаусового (нормального) закону.

---

### **Частина 2: Розділ 2.2 - Застосування методу максимізації поліномів (ММПл)**

Тут вводиться основний інструмент дослідження — метод максимізації поліномів (ММПл) для оцінювання параметрів при неоднаково розподілених даних. Визначається стохастичний поліном $L_{SN}$ та його ключові властивості.

---

Загальна задача полягає в знаходженні оцінок складових вектору інформативного параметру $\theta$ на основі статистичного аналізу статистичних даних $Y$ та $X$.

#### **2.2 Застосування ММПл при знаходженні оцінок векторного параметру при неоднаково розподілених даних**

Для вирішення поставленої задачі регресійного аналізу можна скористатися модифікацією ММПл для статистичного оцінювання векторного параметру при неоднаково розподілених даних. В його основі лежить властивість максимізації функціонала у вигляді стохастичного поліному загального виду

$$
L_{SN} = \sum_{v=1}^{N} \sum_{i=1}^{S} \phi_i(y_v) \int k_{iv}(a) dz - \sum_{i=1}^{S} \sum_{v=1}^{N} \int \Psi_{iv} k_{iv}(a) dz,
\tag{2.2}
$$

Припускається, що випадкові величини $y_v$ описуються послідовністю математичних сподівань

$$
\Psi_{iv} = E\{\phi_i(y_v)\}, \quad i=\overline{1,S}, \quad v=\overline{1,N},
\tag{2.3}
$$

і є двічі диференційовані по параметру $a$, що оцінюється.

Стохастичний поліном $L_{SN}$ виду (2.2) володіє двома основними властивостями [Кунченко]:
1) для будь-якого порядку $S$ при асимптотичному зростанні $v \to \infty$ обсягу вибірки $Y$ поліном $L_{SN}$ як функція параметра $a$ приймає максимум в околиці істинного значення цього параметра;
2) при різних вибірках $Y$ відхилення максимуму полінома $L_{SN}$ від істинного значення параметра $a$ буде мати мінімальну дисперсію (для відповідної порядку поліному $S$).

За аналогією до методу максимальної правдоподібності оцінку параметра $a$ можна знаходити із вирішення рівняння виду

$$
\frac{d}{da} L_{SN} \bigg|_{a=\hat{a}} = \sum_{i=1}^{S} \sum_{v=1}^{N} k_{iv} [\phi_i(y_v) - \Psi_{iv}] \bigg|_{a=\hat{a}} = 0,
\tag{2.4}
$$

Оптимальні коефіцієнти $k_{iv}$, що максимізують функціонал (2.2) знаходяться з розв'язання системи лінійних алгебраїчних рівнянь

$$
\sum_{j=1}^{S} k_{jv} F_{(i,j)v} = \frac{d}{da} \Psi_{iv}, \quad i=\overline{1,S}, \quad v=\overline{1,N},
\tag{2.5}
$$

де $F_{(i,j)v} = \Psi_{(i,j)v} - \Psi_{iv} \Psi_{jv}$, $\Psi_{(i,j)v} = E\{\phi_i(y_v)\phi_j(y_v)\}$, $i, j = \overline{1,S}$.

Даний підхід до оцінювання може бути поширений на випадок знаходження оцінок векторного параметра $\theta = \{a_0, a_1, ..., a_{Q-1}\}$. В такій ситуації необхідно використовувати $Q$ поліномів $L_{SN}^{(p)}$, $p=\overline{0, Q-1}$ загального виду (2.2) для кожної компоненти $a_p$ векторного параметра.

Кожний $p$-ий стохастичний поліном $L_{SN}^{(p)}$ як функція параметра $a_p$ при відомих значеннях інших складових вектору $\theta$ при $n \to \infty$ також має максимум в околиці істинного значення параметра $a_p$. Таким чином, шукані оцінки параметра можуть бути знайдені як розв'язок системи рівнянь виду

$$
f_{SN}^{(p)}(y_v, x_v) = \sum_{i=1}^{S} \sum_{v=1}^{N} k_{iv}^{(p)} [\phi_i(y_v) - \Psi_{iv}] \bigg|_{a_p=\hat{a}_p} = 0, \quad p=\overline{0, Q-1}.
\tag{2.6}
$$

---

### **Частина 3: Розділи 2.3.1 - 2.3.4 - ММПл для лінійної багатофакторної регресії**

Цей великий блок детально розглядає застосування ММПл до найпоширенішого випадку — лінійної регресії. Він послідовно аналізує оцінки для поліномів різного ступеня ($S=1, S=2$), показуючи, як ускладнюються рівняння та які переваги це дає для негаусових помилок.

---

При використанні поліномів степені $S \ge 2$ знаходження РММ-оцінок векторного параметру у переважній більшості випадків (аналогічно до ситуації із використанням MLE) потребує застосування чисельних методів розв'язку систем нелінійних рівнянь. Зокрема, часто використовується підхід, який базується на ітераційній чисельній процедурі Ньютона-Рафсона. В його основі лежить принцип лінеаризації шляхом розкладу лівої частини кожного нелінійного рівняння системи (2.6) в ряд Тейлора в околі істинного значення вектору $\theta$. Обмежившись першими двома членами ряду можна записати у матричній формі лінійну систему

$$
\hat{\theta}_{(S)}^{(k+1)} = \hat{\theta}_{(S)}^{(k)} - [Z_S(Y/\hat{\theta}^{(k)})]^{-1} F_S(Y/\hat{\theta}^{(k)}),
\tag{2.7}
$$

яка може бути використана для ітераційного пошуку оціночних значень. Для отримання системи (2.7) необхідно обчислити матрицю-стовпець $F_S(Y/\hat{\theta}^{(k)})$, що складена з елементів лівої частини кожного нелінійного рівняння системи (2.6) та квадратну матрицю $Z_S(Y/\hat{\theta}^{(k)}) = H_S(Y/\hat{\theta}^{(k)}) + J_S(\hat{\theta}^{(k)})$ із складовими

$$
H_{SN}^{(p,q)} = \sum_{i=1}^{S} \frac{\partial}{\partial a_q} k_{iv}^{(p)} \left[ \sum_{v=1}^{N} \phi_i(y_v) - N \Psi_{iv} \right]
\tag{2.8}
$$

і елементами матриці кількості добутої інформації $J_S(\hat{\theta}^{(k)})$ виду

$$
J_{SN}^{(p,q)} = \sum_{v=1}^{N} \sum_{i=1}^{S} k_{i,v}^{(p)} \frac{\partial}{\partial a_q} \Psi_{iv}, \quad p,q=\overline{0,Q-1}.
\tag{2.9}
$$

Для старту ітераційної процедури припускається наявність деякого початкового наближення $\theta^{(1)} = \{a_0^{(1)}, a_1^{(1)}, ..., a_{Q-1}^{(1)}\}^T$ в якості якого можуть бути обрані «грубі» оцінки, знайдені більш простим естиматором або лінійні РММ-оцінки при степені $S=1$.

#### **2.3 Знаходження ММПл-оцінок параметрів лінійної багатофакторної регресії**

...

##### **2.3.1. Постановка задачі оцінювання параметрів лінійної регресії**

Модифікуємо загальну постановку задачі із підрозділу 2.1 для випадку лінійної регресійної моделі. У цьому разі регресійна модель спостережень описує залежність значень цільової змінної $Y = \{y_1, y_2, ..., y_N\}$ у вигляді лінійної комбінації між собою незалежних змінних

$$
y_v = a_0 + \sum_{p=1}^{Q-1} a_p x_{p,v} + \xi_v = \sum_{p=0}^{Q-1} a_p x_{p,v} + \xi_v, \quad v=\overline{1,N}.
\tag{2.10}
$$

...

##### **2.3.2. Застосування метода максимізації поліномів при степені S=1 для оцінювання параметрів лінійної регресії**

...
При використанні стохастичного полінома порядку $S=1$ ММПл-оцінки елементів векторного параметра $\theta$ детермінованої складової регресійної моделі (2.10) можуть бути знайдені з рішення системи $Q$ рівнянь виду:

$$
\sum_{v=1}^{N} k_{1,v}^{(p)} \left[ y_v - \sum_{p=0}^{Q-1} a_p x_{p,v} \right] = 0, \quad p=\overline{0,Q-1}.
\tag{2.13}
$$

де оптимальні коефіцієнти $k_{1,v}^{(p)}$ знаходяться як розв'язання системи лінійних алгебраїчних рівнянь (2.5), ... і які можна представити у вигляді

$$
k_{1,v}^{(p)} = \frac{\partial}{\partial a_p} \left[ \sum_{p=0}^{Q-1} a_p x_{p,v} \right] = \frac{x_{p,v}}{\mu_2}, \quad p=\overline{0,Q-1}.
$$

...
Відзначимо, що система (2.15) для знаходження ММПл-оцінок $\hat{\theta}_{(1)}$ параметрів лінійної регресії (2.10) при $S=1$ є тотожною відповідній лінійній системі МНК-оцінок (1.3). Такі оцінки є оптимальними (за критерієм мінімуму дисперсії) лише для ситуації, коли помилки регресійній моделі мають гаусовий розподіл.

##### **2.3.3. Застосування метода максимізації поліномів при степені S=2 для оцінювання параметрів лінійної регресії**

...
сформуємо систему із $Q$ рівнянь для пошуку ММПл-оцінок $\hat{\theta}_{(2)}$ ... у вигляді:

$$
\sum_{v=1}^{N} \left\{ k_{1,v}^{(p)} \left[ y_v - \sum_{p=0}^{Q-1} a_p x_{p,v} \right] + k_{2,v}^{(p)} \left[ (y_v)^2 - \left( \sum_{p=0}^{Q-1} a_p x_{p,v} \right)^2 + \mu_2 \right] \right\} = 0, \quad p=\overline{0,Q-1},
\tag{2.16}
$$

де оптимальні коефіцієнти $k_{i,v}^{(p)}, i=\overline{1,2}$ ... можуть бути представлені як функції, що залежать від центральних моментів випадкової складової регресійної моделі:

$$
k_{1,v}^{(p)} = \frac{\mu_4 - \mu_2^2 + 2\mu_3 \sum_{p=0}^{Q-1} a_p x_{p,v}}{\mu_2(\mu_4 - \mu_2^2) - \mu_3^2} x_{p,v}, \quad k_{2,v}^{(p)} = -\frac{\mu_3}{\mu_2(\mu_4 - \mu_2^2) - \mu_3^2} x_{p,v}.
\tag{2.17}
$$



---



### **Частина 4: Розділ 2.6 - Адаптивне оцінювання та алгоритм**

Ця частина є кульмінацією розділу. Вона об'єднує попередні результати в єдиний адаптивний алгоритм. Описується покрокова процедура, яка починається з простої МНК-оцінки, аналізує властивості залишків (гаусовість, симетрія) і, залежно від результатів, застосовує більш складні ММПл-оцінки ($S=2$ або $S=3$) для уточнення результатів. Блок-схема візуалізує цей процес.

---

#### **2.6 Адаптивне оцінювання параметрів регресії із застосуванням методу максимізації поліномів**

Отримані результати попередніх підрозділів свідчать про доцільність застосування методу максимізації поліномів лише при відмінності розподілу випадкової складової регресійних моделей від гаусового закону. ... для отримання як ММПл так і ММП-оцінок необхідна додаткова апріорна інформація про властивості помилок регресійної моделі, яка в реальних умовах зазвичай відсутня.

Відомо, що ... може бути застосовано адаптивний підхід. Адаптивність розуміється у тому сенсі, що застосовується ряд послідовних уточнюючих кроків.
1.  **На першому етапі** здійснюється оцінювання параметрів ... звичайним МНК.
2.  **Після видалення детермінованої складової** здійснюється ідентифікація типу випадкової складової ... та знаходять оцінки її параметрів.
3.  **На третьому етапі** знаходять уточнені (адаптивні) оцінки ... із урахуванням ймовірнісних властивостей похибок.

З урахуванням вище викладеного, узагальнимо алгоритм (графічне представлення блок-схеми якого наведено на рис.2.1), для знаходження адаптивних РММ-оцінок параметрів регресії у вигляді:

**Рисунок 2.1. Блок-схема алгоритму адаптивного оцінювання параметрів регресії методом максимізації поліномів**

*   **Крок 1:** знаходження лінійних МНК-оцінок параметрів регресії.
*   **Крок 2:** формування регресійних залишків і знаходження апостеріорних оцінок моментів до 4-го порядку.
*   **Крок 3:** перевірка гіпотези про гаусовий розподіл регресійних МНК-залишків (в разі її підтвердження алгоритм завершується).
*   **Крок 4:** перевірка гіпотези про симетрію розподілу регресійних МНК-залишків (в разі її підтвердження здійснюється перехід до кроку 6).
*   **Крок 5:** знаходження ММПл-оцінок з використанням полінома ступеня $S=2$ і завершення алгоритму.
*   **Крок 6:** знаходження оцінок моментів 6-го порядку регресійних МНК-залишків.
*   **Крок 7:** знаходження ММПл-оцінок з використанням полінома ступеня $S=3$ і завершення алгоритму.

---

### **Частина 5: Розділ 2.7 - Висновки**

Завершальна частина, яка підсумовує ключові результати, отримані в розділі. Вона чітко формулює основні тези щодо застосовності ММПл для різних ступенів полінома та умов розподілу помилок.

---

#### **2.7 Висновки**

В даному розділі на основі використання апарату стохастичних поліномів Кунченка та часткового опису статистиками вищих порядків здійснено синтез обчислювальних методів та алгоритмів адаптивного оцінювання параметрів регресійних моделей лінійного, поліноміального і нелінійного типу. На основі сукупності отриманих результатів можна зробити наступні висновки:

1.  Загальна задача знаходження оцінок ... може бути зведена до розв'язання системи нелінійних (степеневих) стохастичних рівнянь із застосуванням чисельних ітераційних процедур Ньютона-Рафсона.
2.  При використанні стохастичних поліномів степені $S=1$ системи рівнянь методу максимізації поліному є еквівалентними відповідним системам, що отримуються із застосуванням методу найменших квадратів...
3.  Використання стохастичних поліномів при степені $S=2$ для оцінювання параметрів регресії є доцільним лише для випадку асиметричних негаусових розподілів регресійних помилок.
4.  При симетричному характері розподілу регресійних помилок для оцінювання параметрів доцільним є застосування стохастичних поліномів лише при степені $S=3$.
5.  За відсутності апріорної інформації про властивості випадкової складової ... може бути застосований адаптивний підхід, заснований на використанні апостеріорних оцінок статистик вищих порядків регресійних МНК-залишків.